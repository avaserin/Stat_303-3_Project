{
 "cells": [
  {
   "cell_type": "raw",
   "id": "33dd6c4c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Project proposal\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116f49b",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. Please answer the following questions as part of your project proposal.\n",
    "\n",
    "2. Write your answers in the *Markdown* cells of the Jupyter notebook. You don't need to write any code, but if you want to, you may use the *Code* cells.\n",
    "\n",
    "3. Use [Quarto](https://quarto.org/docs/output-formats/html-basics.html) to print the *.ipynb* file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: `quarto render filename.ipynb --to html`. Submit the HTML file.\n",
    "\n",
    "4. The project proposal is worth 8 points, and is due on **18th April 2023 at 11:59 pm**. \n",
    "\n",
    "5. You must make one submission as a group, and not individually.\n",
    "\n",
    "6. Maintaining a GitHub repository is optional, though encouraged for the project.\n",
    "\n",
    "7. Share the link of your project's GitHub repository [here](https://docs.google.com/spreadsheets/d/1khao3unpj_vsx4kOSg_Zzo77YK1UWL2w73Oa0aAirOo/edit#gid=0) (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ea9bb",
   "metadata": {},
   "source": [
    "# 1) Team name\n",
    "Mention your team name.\n",
    "\n",
    "*(0 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3583e3f",
   "metadata": {},
   "source": [
    "Python Princesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fccc9b",
   "metadata": {},
   "source": [
    "# 2) Member names\n",
    "Mention the names of your team members.\n",
    "\n",
    "*(0 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6d1b2",
   "metadata": {},
   "source": [
    "Avanti Parkhe, Ava Serin, Emily Zhang, Ada Zhong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a6528",
   "metadata": {},
   "source": [
    "# 3) Link to the GitHub repository (optional)\n",
    "Share the link of the team's project repository on GitHub.\n",
    "\n",
    "Also, put the link of your project's GitHub repository [here](https://github.com/avaserin/Stat_303-3_Project).\n",
    "\n",
    "We believe there is no harm in having other teams view your GitHub repository. However, if you don't want anyone to see your team's work, you may make the repository *Private* and add your instructor and graduate TA as *Colloborators* in it.\n",
    "\n",
    "*(0 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1a490",
   "metadata": {},
   "source": [
    "# 4) Topic\n",
    "Mention the topic of your course project.\n",
    "\n",
    "*(0 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf1804",
   "metadata": {},
   "source": [
    "Predicting median house prices of Boston neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403cdfb",
   "metadata": {},
   "source": [
    "# 5) Problem statement\n",
    "\n",
    "*(4 points)*\n",
    "\n",
    "Explain the problem statement. The problem statement must include:\n",
    "\n",
    "## 5a) The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522a1e3",
   "metadata": {},
   "source": [
    "We are trying to predict median house price of Boston neighborhoods/towns based on a number of features. We were motivated to analyze this problem as we are college students nearing graduation who want to learn about what factors influence house prices, as finding housing after graduation is a complex process. One member of our group is moving to Boston post-grad, so we figured this dataset would be fitting to our particular needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0dee45",
   "metadata": {},
   "source": [
    "## 5b) Type of response\n",
    "Is it about predicting a continuous response or a binary response or a combination of both?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c9eab",
   "metadata": {},
   "source": [
    "We are predicting a continuous response (median house price)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8cb75b",
   "metadata": {},
   "source": [
    "## 5c) Performance metric\n",
    "How will you assess model accuracy?\n",
    "\n",
    "  - If it is a classification problem, then which measure(s) will you optimize for your model – precision, recall, false negative rate (FNR), accuracy, ROC-AUC etc., and why?\n",
    "  - If it is a regression problem, then which measure(s) will you optimize for your model – RMSE (Root mean squared error), MAE (mean absolute error), maximum absolute error etc., and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a160b",
   "metadata": {},
   "source": [
    "We currently plan to optimize RMSE. As RMSE penalizes larger errors more, it would reduce the outliers in our prediction. Large errors in our prediction impacts consumer purchasing decisions, given how high housing prices are, as well as the accuracy of information provided to stakeholders. We want to penalize larger errors to minimize these negative consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57701e9e",
   "metadata": {},
   "source": [
    "## 5d) Naive model accuracy\n",
    "What is the accuracy of the naive model (Standard deviation of response in case of continuous response / proportion of the majority class in case of classification model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c786db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.35)\n",
    "\n",
    "# sklearn has 100s of models - grouped in sublibraries, such as linear_model\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "# sklearn has many tools for cleaning/processing data, also grouped in sublibraries\n",
    "# splitting one dataset into train and test, computing cross validation score, cross validated prediction\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
    "\n",
    "#sklearn module for scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#sklearn modules for computing the performance metrics\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, r2_score, \\\n",
    "roc_curve, auc, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "data = pd.read_csv('boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d49a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the predictors and response - THIS IS HOW ALL SKLEARN OBJECTS ACCEPT DATA (different from statsmodels)\n",
    "y = data.MEDV\n",
    "X = data.drop(\"MEDV\", axis = 1)\n",
    "\n",
    "# Creating training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)\n",
    "\n",
    "# With linear/logistic regression in scikit-learn, especially when the predictors have different orders \n",
    "# of magn., scaling is necessary. This is to enable the training algo. which we did not cover. (Gradient Descent)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) # Do NOT refit the scaler with the test data, just transform it.\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14188a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of response variable:  9.188011545278206\n"
     ]
    }
   ],
   "source": [
    "y_std = np.std(y)\n",
    "print(\"Standard deviation of response variable: \", y_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd1f68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on train data:\n",
      "Linear regression: 4.534072623998566\n",
      "RMSE on test data:\n",
      "Linear regression: 5.214447779533273\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE on train data:\")\n",
    "print(\"Linear regression:\", np.sqrt(mean_squared_error(lr_model.predict(X_train),y_train)))\n",
    "\n",
    "print(\"RMSE on test data:\")\n",
    "print(\"Linear regression:\", np.sqrt(mean_squared_error(lr_model.predict(X_test),y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f573ff3",
   "metadata": {},
   "source": [
    "Standard deviation of response variable:  9.188011545278206\n",
    "\n",
    "RMSE on train data:\n",
    "Linear regression: 4.534072623998566\n",
    "\n",
    "RMSE on test data:\n",
    "Linear regression: 5.214447779533273"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2d4f2",
   "metadata": {},
   "source": [
    "# 6) Data\n",
    "\n",
    "## 6a) Source\n",
    "What data sources will you use, and how will the data help solve the problem? Explain.\n",
    "If the data is open source, share the link of the data.\n",
    "\n",
    "*(0.5 point)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339dcd7",
   "metadata": {},
   "source": [
    "We are using a dataset published by the University of Toronto's Data for Evaluating Learning in Valid Experiments website. It contains housing information collected by the U.S Census Service regarding housing in the area of Boston. The dataset came as a .tar.gz file. We then converted this file into a csv file. The [documentation](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and [data](https://www.cs.toronto.edu/~delve/data/boston/desc.html) are linked here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bee818",
   "metadata": {},
   "source": [
    "## 6b) Response & predictors\n",
    "What is the response, and mention some of the predictors.\n",
    "\n",
    "*(0.5 point)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bec2cb",
   "metadata": {},
   "source": [
    "The response variable is the variable MEDV, median value of owner-occupied homes is 1000’s, and some of the predictors are CRIM (per capita crime rate by town), RM (average number of rooms per dwelling), PTRATIO (pupil-teacher ratio by town), and Tax (full-value-property-tax-rate per $10,000). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb561c3",
   "metadata": {},
   "source": [
    "## 6c) Size\n",
    "What is the number of continuous predictors, categorical predictors, and observations in your dataset(s). If you are using multiple datasets, please provide the information for each dataset. When counting predictors, count only those that have sufficient non-missing values, and will be useful.\n",
    "\n",
    "*(1 point)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55a216",
   "metadata": {},
   "source": [
    "Number of continuous predictors: 14\n",
    "\n",
    "Number of categorical predictors: 0\n",
    "\n",
    "Number of observations in the dataset: 506"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450286f",
   "metadata": {},
   "source": [
    "# 7) Exisiting solutions\n",
    "Are there existing solutions of your problem? Almost all Kaggle datasets have exisiting solutions. If yes, then how do you plan to build up on those solutions? **What is the highest model accuracy / performance achieved in the existing solutions?**\n",
    "\n",
    "*(1 point)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66e8f08",
   "metadata": {},
   "source": [
    "While we didn’t find the dataset on Kaggle, there are existing solutions out there. We plan on creating more in depth models than the existing solutions on Kaggle and trying out different accuracy metrics. Some of the existing solutions on Kaggle look into cross validated R2 scores. We are planning on looking into RMSE as a way to distinguish our project. We will also tune different models with various C values and interactions (poly(), interaction terms, etc…). In the existing solutions, they measured cross validated R2 scores. The highest score for this was around 0.85. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a845f",
   "metadata": {},
   "source": [
    "# 8) Stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c782c",
   "metadata": {},
   "source": [
    "Who are the stakeholders, and how will your project benefit them? Explain.\n",
    "\n",
    "*(1 point)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f8441",
   "metadata": {},
   "source": [
    "Our primary stakeholders are current and prospective Boston residents, local realtors, and Boston housing authority. Our project will benefit current and prospective Boston residents because they will be able to better predict house prices and budget more efficiently. Local realtors will be able to market house prices more accurately and have better information on what houses to present to certain clients. The Boston housing authority can benefit by having more information on what neighborhoods have more affordable housing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
